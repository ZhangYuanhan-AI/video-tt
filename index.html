
<!doctype html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
	<meta name="google-site-verification" content="eGDOZ_6azobM9Vcl7r072IFo1FJ-TfNvGkmz6YbLCLo" />
	
	<!-- Preload critical resources -->
	<link rel="preload" href="https://fonts.cdnfonts.com/css/corbel" as="style">
	<link rel="preload" href="./html_pages/resources/bootstrap.min.css" as="style">
	
	<!-- Font imports -->
	<link href="https://fonts.cdnfonts.com/css/corbel" rel="stylesheet">
	
	<!-- CSS -->
	<link href="./html_pages/resources/bootstrap.min.css" rel="stylesheet">
	<link href="./html_pages/resources/main.css" rel="stylesheet">
	
	<!-- JavaScript -->
	<script src="js/jquery-2.1.3.min.js" defer></script>
	<script src="./html_pages/resources/main.js" defer></script>
	
	<title id="title">Video Turing Test</title>
</head>
	
<body data-new-gr-c-s-check-loaded="14.1110.0" data-gr-ext-installed="">
	<header>
		<nav>
			<a class="h7 pt-10" style="margin: 5px; white-space: nowrap; font-weight: 900; font-size: 1.7rem;" href="#">Video-TT</a>
		</nav>
		<nav>
			<a href="#abstract">Abstract</a>
			<a href="#video">Demo</a>
			<a href="#annotation">Annotation</a>
			<a href="#statistic">Statistic</a>
			<a href="#performance">Performance</a>
			<a href="#acknowledgement">Acknowledgement</a>
		</nav>
	</header>

	<section class="jumbotron text-center pb-2" id="Video-TT">
		<div class="video-background">
			<video playsinline="playsinline" autoplay="autoplay" muted="muted" loop="loop">
				<source src="assets/header_video.mp4" type="video/mp4">
			</video>
		</div>
		<div class="container">
			<br><br><br>
			<h1 class="jumbotron-heading" style="font-size: 6rem; font-weight: 900;">Video Turing Test</h1>
			<h5 class="pt-1" style="font-size: 2rem; font-weight: normal">Video Comprehension and Reasoning Benchmark with Complex Visual Narratives			</h5>
			<br>
			<a href="https://zhangyuanhan-ai.github.io/" target="_blank" rel="noopener noreferrer" style="font-size: 1.2rem; color: white !important;">Yuanhan Zhang*<a>,
			<a href="https://openreview.net/profile?id=~Yunice_Chew1" target="_blank" rel="noopener noreferrer" style="font-size: 1.2rem; color: white !important;">Yunice Chew*</a>,
			<a href="https://scholar.google.com/citations?user=kMui170AAAAJ&hl=zh-CN" target="_blank" rel="noopener noreferrer" style="font-size: 1.2rem; color: white !important;">Yuhao Dong</a>,
			<a href="https://liuziwei7.github.io/" target="_blank" rel="noopener noreferrer" style="font-size: 1.2rem; color: white !important;">Ziwei Liu</a>
			<br><br>
			<a style="font-size: 1.2rem; color: white !important;"><i>* Equal contribution</i></a>
			<br><br>
			<a style="font-size: 1.2rem; color: white !important;"><i>ICCV 2025</i></a>
			<br><br>
			<a href="https://liuziwei7.github.io/team.html" target="_blank" rel="noopener noreferrer">
				<div style="display: inline-block; background: radial-gradient(ellipse at center, rgba(20, 20, 20, 1) 0%, rgba(20, 20, 20, 0) 80%); border-radius: 20px; padding: 10px;">
					<img class="paper-btn" src="./assets/ntu.png" style="height: 80px; width: auto; display: block;">
				</div>
			</a>
			<br><br><br>
			<div style="display: flex; justify-content: center; gap: 15px; margin-top: 10px;">
				<!-- Paper Button -->
				<a href="https://arxiv.org/abs/2501.13826" target="_blank" rel="noopener noreferrer" class="jumbotron-button">
					<img src="./assets/arxiv.svg" alt="" style="height: 20px; width: auto;">
					Paper
				</a>
				<!-- Dataset Button -->
				<a href="https://huggingface.co/datasets/lmms-lab/VideoMMMU" target="_blank" rel="noopener noreferrer" class="jumbotron-button">
					<img src="./assets/huggingface.png" alt="" style="height: 20px; width: auto;">
					Dataset
				</a>
					<!-- Example Button -->
					<a href="./more_samples.html" rel="noopener noreferrer" class="jumbotron-button">
						<img src="./assets/movie.svg" alt="" style="height: 20px; width: auto;">
						Examples
				</a>
			</div>
		</div>
	
	</section>



	<section id="abstract">
		<div class="container">
			<h1 class="jumbotron-heading">Abstract</h1>
			<div style="justify-content: space-around; flex-wrap: wrap; margin-top: 30px">
				<div class="image-item">
					<img src="./assets/teaser.png" class="zoomable-image" style="margin-top: -20px; width: 100%; max-width: 8000px; height: auto;">
				</div>
			</div>
			
            <p>We introduce <b>the Video Turing Test (Video-TT)</b>, a benchmark designed to assess if video LLMs can interpret real-world videos as effectively as humans. <b>Video-TT</b> <b>1)</b> differentiates  between errors due to inadequate frame sampling and genuine gaps in understanding complex visual narratives, and <b>2)</b> evaluates robustness against natural adversarial questions. <b>Video-TT</b> comprises 1,000 YouTube Shorts videos, each with one open-ended question and four adversarial questions that probe visual and narrative complexity. Our evaluation shows a significant gap between video LLMs and human performance, underscoring the need for benchmarks like <b>Video-TT</b> to advance video understanding.</p>
		</div>
		
	</section>

	<section id="video" style="margin-top: 20px;">
		<div class="container">
			<h1 class="jumbotron-heading">Video Demo</h1>
		  <div class="youtube-container" style="position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden; max-width: 100%; height: auto;">
			<iframe 
			  src="https://www.youtube.com/embed/tnSzl-m4KXA" 
			  title="YouTube video player" 
			  frameborder="0" 
			  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" 
			  allowfullscreen 
			  style="position: absolute; top: 0; left: 0; width: 100%; height: 100%;">
			</iframe>
		  </div>
		</div>
		
	</section>

	<section id="annotation" style="margin-top: 20px;">
		<div class="container">
		  <h1 class="jumbotron-heading">Data Annotation Pipeline</h1>
	  
		  <div style="display: flex; justify-content: center; flex-wrap: wrap; margin-top: 30px;">
			<div class="image-item">
			  <img 
				src="assets/annotation_pipeline.png" 
				alt="Data Annotation Pipeline Diagram" 
				class="zoomable-image" 
				style="margin-top: -20px; width: 100%; height: auto;"
			  >
			</div>
		  </div>
	  
		  <div style="text-align: left; margin-top: 40px;">
			<p><strong>The dataset is built through a multi-step annotation and verification pipeline:</strong></p>
			<ul class="info-list">
				<li>
					<strong style="color: #E97132;">Ensuring Complexity:</strong> (Visual Complexity) Measures how visually challenging a video is, based on factors like unclear or unusual content, fast motion, complex object arrangements, and visual illusions that hinder recognition. (Narrative Complexity) Reflects how cognitively demanding the storyline is, including elements like plot twists, montage-style editing, subtle technical manipulations, and reliance on world knowledge for full comprehension.
				</li>
				<li>
					<strong style="color: #E97132;">Primary Question Annotation:</strong> Annotators select videos and create QA pairs requiring either <em>visual</em> or <em>narrative complexity</em>. A question is retained only if at least one top model (GPT-4o, LLaVA-Video, Qwen2.5-VL) fails to answer it correctly.
			  	</li>
				<li>
					<strong style="color: #4EA72E;">Answer & Rationale:</strong> Annotators provide the correct answer, a detailed reasoning process, and critique of incorrect model responses.
				</li>
				<li>
					<strong style="color: #4EA72E;">Sampling Check:</strong> Questions must be answerable from 80 uniformly sampled frames, ensuring reliance on visual rather than auditory cues.
				</li>
				<li>
					<strong style="color: #4E95D9;">Adversarial Question Expansion:</strong> Annotators create four challenging variants per primary question based on model failures, with minimal edits to the original answer and rationale.
				</li>
				<li>
					<strong style="color: #b7b7b7;">Alignment Check:</strong> A consensus-based process among three annotators ensures consistency. Questions without agreement are discarded.
				</li>
			</ul>
		  </div>
		</div>
	  </section>
	  
	<section id="statistic" style="margin-top: 20px;">
		<div class="container">
			<h1 class="jumbotron-heading">Dataset Statistics</h1>
			<div style="justify-content: space-around; flex-wrap: wrap; margin-top: 30px">
				<div class="image-item">
					<img src="./assets/statistics.jpg" class="zoomable-image" style="margin-top: -20px; width: 100%; max-width: 8000px; height: auto;">
				</div>
			</div>
			
            <p>The dataset includes 5,000 question-answer pairs across 1,000 videos. Questions were first grouped by reasoning level—element, event, or plot—based on video content. They were further categorized by the type of inquiry (e.g., Attributes, Localization). When a specific complexity factor appeared frequently within a category (e.g., over 50 instances), it was promoted to a sub-category (e.g., Element Attributes–Illusion). In total, 18 distinct question types are identified.</p>
		</div>
		
	</section>

	<section id="performance" style="margin-top: 20px;">
		<div class="container text-left">
			<h1 class="jumbotron-heading">Performance</h1>
			<div style="justify-content: space-around; flex-wrap: wrap; margin-top: 30px">
				<div class="image-item">

					<img src="./assets/performance.png" class="zoomable-image" style="margin-top: -20px; width: 100%; max-width: 8000px; height: auto;">
				</div>
				
				<p>
					Among video-language models, performance varies widely. While open-source models like InternVL-2.5-8B perform well on straightforward questions (65.7% on Correctly-Led), they struggle with misleading prompts (24.5% on Wrongly-Led). LLaVA-Video-72B emerges as the strongest open-source model overall. Proprietary models such as GPT-4o and Gemini Pro outperform most open-source models, with GPT-4o showing greater robustness to misleading prompts (67.5% Correctly-Led, 39.8% Wrongly-Led), though still far behind human-level reasoning. Notably, LLaVA-Video-72B approaches GPT-4o's accuracy in multiple-choice settings, but falls short on primary open-ended questions—highlighting both a limitation of current open-source systems and a bias in existing benchmarks that overemphasize multiple-choice formats.
					
					Regarding natural adversarial robustness, the table reveals that humans remain the gold standard with 64.4% accuracy. GPT-4o ranks highest among models at 36.0%, but still significantly lags behind. Open-source models like InternVL-2.5-7B perform poorly (10.9%), with even larger variants offering minimal gains. These results underscore the challenge of building models that can resist adversarial perturbations and the need for more rigorous benchmarks targeting open-ended and robustness-centric tasks.
					
				</p>
			</div>
		</div>
	</section>



	<section id="acknowledgement" style="margin-top: 20px;">
		<div class="container text-left">
			<h1 class="jumbotron-heading">Acknowledgement</h1>
			<div style="display: flex; justify-content: space-around; flex-wrap: wrap; margin-top: 20px">
				<p style="font-weight: 200">
					We sincerely thank to everyone who contributed to the meaningful discussions,
					and also extend our gratitude to TikTok Pte. Ltd. for providing the computational resources and fostering a conducive research environment. &#129303; &#128591;
				</p>
			</div>
		</div>
	</section>



	<!-- The Modal -->
	<div id="imageModal" class="modal">
		<span class="close">&times;</span>
		<img class="modal-content" id="modalImage">
	</div>

</body>
</html>
